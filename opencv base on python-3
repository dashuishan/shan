#######################
#####
#####  CAPTER 9
#####
#######################
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 转为灰度图像
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

    # 生成关键点
    kps = []
    key_points = np.random.randint(0, 512, 200).reshape((100, 2))
    # print(key_points)
    # exit()
    for point in key_points:
        kps.append(cv.KeyPoint(float(point[0]), float(point[1]), 1))#这里程序中代码错误，有对于cv.KeyPoint()，前两个参数需要用float类型。

    # 绘制关键点
    image_result = cv.drawKeypoints(image, kps, None, (), cv.DRAW_MATCHES_FLAGS_DEFAULT)
    gray_result = cv.drawKeypoints(gray, kps, None, (), cv.DRAW_MATCHES_FLAGS_DEFAULT)

    # 展示结果
    cv.imshow('Color Result', image_result)
    cv.imshow('Gray Result', gray_result)
    cv.waitKey(0)
    cv.destroyAllWindows()
#################这里的角点检测实在看不出是什么，因为这些角点都是随机生成的，根本不是关键点。
import cv2 as cv
import numpy as np
import sys

if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 转为灰度图像
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

    # 计算Harris系数
    harris = cv.cornerHarris(gray, 2, 3, 0.04, borderType=cv.BORDER_DEFAULT)

    # 对Harris进行归一化便于进行数值比较
    harris_nor = cv.normalize(harris, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)
    harris_nor = harris_nor.astype('uint8')

    # 寻找Harris角点
    kps = []
    for i in np.argwhere(harris_nor > 125):#与125阈值相互比较，大于就保存下来。125这个值的选择，我认为与255的最大值关系较大。其实也可以该换成别的阈值。
        kps.append(cv.KeyPoint(float(i[1]), float(i[0]), 1))#则合理的cv.KeyPoint()里面是float属性。

    # 绘制角点
    result = cv.drawKeypoints(image, kps, None)

    # 展示结果
    cv.imshow('R', harris_nor)
    cv.imshow('Harris KeyPoints', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
#出Hams角点主要集中在图像人物的头发和帽子区域，因为这两个区域中线段的交点较多。
########################################################################梯度协方差矩阵的两个特征值与Harris角点判定相关。
import cv2 as cv
import numpy as np
import sys

if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()
    # 将图像进行拷贝便于使用cv.drawKeypoints()函数绘制角点
    image1 = image.copy()

    # 转为灰度图像，并将数据类型转换为float32
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

    # 检测Shi-Tomasi角点
    corners = cv.goodFeaturesToTrack(gray, 500, 0.01, 0.04)
    corners = np.int0(corners)

    # 绘制角点（Mode 1：使用cv.circle()函数）
    kps = []
    for corner in corners:
        x, y = corner.ravel()#ravel()方法将数组维度拉成一维数组
        cv.circle(image, (x, y), 3, (0, 255, 255), -1)

        # 将角点转化为KeyPoint类，以便于方法2的绘制
        kps.append(cv.KeyPoint(float(x), float(y), 1))

    # 绘制角点（Mode 2：使用cv.drawKeypoints()函数）
    result = cv.drawKeypoints(image1, kps, None, (), cv.DRAW_MATCHES_FLAGS_DEFAULT)

    # 展示结果
    cv.imshow('Shi-Tomasi KeyPoints(Mode 1)', image)
    cv.imshow('Shi-Tomasi KeyPoints(Mode 2)', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
#########################################Shi-Tomasi角点检测方法是cornerHarris的一种。也是矩阵梯度协方差矩阵特征值的最小值，且能够使用 cv.drawKeypoints()函数直接画出来，不需要
阈值的限定了。
import cv2 as cv
import sys

if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 创建SURF对象
    surf = cv.xfeatures2d.SURF_create(500, 4, 3, True, False)

    # 计算SURF特征点
    kps = surf.detect(image, None)

    # 计算SURF描述子
    descriptions = surf.compute(image, kps)

    # 绘制SURF特征点
    image1 = image.copy()
    # 不含角度和大小
    image = cv.drawKeypoints(image, kps, image, ())
    # 包含角度和大小
    image1 = cv.drawKeypoints(image1, kps, image1, (), cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # 展示结果
    cv.imshow('SURF KeyPoints', image)
    cv.imshow('SURF KeyPoints(with Angle and Size)', image1)
    cv.waitKey(0)
    cv.destroyAllWindows()
##########################################################计算SURF特征点。但是cv.xfeatures2d.SURF_create()在新的版本中并不支持调用。SURF是SIFT的变形。方便提升SIFT的效率。
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 创建ORB对象
    orb = cv.ORB_create(500, 1.2, 8, 31, 0, 2, cv.ORB_HARRIS_SCORE, 31, 20)

    # 计算ORB特征点
    kps = orb.detect(image, None)#因为是继承的关系，所以是可以使用detect和compute方法的。

    # 计算ORB描述子
    descriptions = orb.compute(image, kps)

    # 绘制ORB特征点
    image1 = image.copy()
    # 不含角度和大小
    image = cv.drawKeypoints(image, kps, image, ())
    # 包含角度和大小
    image1 = cv.drawKeypoints(image1, kps, image1, (), cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # 展示结果
    cv.imshow('ORB KeyPoints', image)
    cv.imshow('ORB KeyPoints(with Angle and Size)', image1)
    cv.waitKey(0)
    cv.destroyAllWindows()
##################################################ORB存在的意义是SURF和SIFT的速率太低，而且专利限制也无法进行相关运算。这个是计算两个像素点之间汉明距的方法，而SURE和SIFT是计算欧式距离的方法。
####两个整数之间的 汉明距离 指的是这两个数字对应二进制位不同的位置的数目。
import cv2 as cv
import sys


if __name__ == '__main__':
    # 读取图像
    image1 = cv.imread('./images/box.png')
    image2 = cv.imread('./images/box_in_scene.png')
    if image1 is None or image2 is None:
        print('Failed to read box.png or box_in_scene.png.')
        sys.exit()

    # 创建ORB对象
    orb = cv.ORB_create(1000, 1.2, 8, 31, 0, 2, cv.ORB_HARRIS_SCORE, 31, 20)#暴力匹配使用的是ORB的方法，因此是可以运行的。

    # 分别计算image1，image2的ORB特征点和描述子
    kps1, des1 = orb.detectAndCompute(image1, None, None)#分别计算两张图片的ORB特征和描述子
    kps2, des2 = orb.detectAndCompute(image2, None, None)

    # 特征点匹配
    # 创建BFMatcher对象
    bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)#crossCheck意思是使用了交叉检查的方法
    matches = bf.match(des1, des2)

    # 输出匹配结果中最大值和最小值
    matches_list = []
    for match in matches:
        matches_list.append(match.distance)
    max_dist = max(matches_list)
    min_dist = min(matches_list)

    # 设定阈值，筛选出合适的匹配点对
    good_matches = []
    for match in matches:
        if match.distance <= max(2.0 * min_dist, 20.0):#距离最小汉明距离的2倍和距离20中的较大值作为优化匹配的阈值对特征点对进行筛选。
            good_matches.append(match)

    # 输出匹配成功的特征点数目
    print('匹配成功的特征点数目为：{}，筛选后的特征点数目为：{}'.format(len(matches), len(good_matches)))

    # 绘制筛选前后的匹配结果
    result1 = cv.drawMatches(image1, kps1, image2, kps2, matches, None)#cv.drawMatches()画出来匹配的线段
    result2 = cv.drawMatches(image1, kps1, image2, kps2, good_matches, None)

    # 展示结果
    cv.imshow('Matches', result1)
    cv.imshow('Good Matches', result2)
    cv.waitKey(0)
    cv.destroyAllWindows()
    ################这里是暴力匹配的案例，使用的ORB的匹配 然后使用了cv.BFMatcher()对比两张图。并使用cv.drawMatches()画出匹配的线段。
import cv2 as cv
import sys


if __name__ == '__main__':
    # 读取图像
    image1 = cv.imread('./images/box.png')
    image2 = cv.imread('./images/box_in_scene.png')
    if image1 is None or image2 is None:
        print('Failed to read box.png or box_in_scene.png.')
        sys.exit()

    # 创建ORB对象
    surf = cv.xfeatures2d.SURF_create(500, 4, 3, True, False)

    # 分别计算image1，image2的ORB特征点和描述子
    kps1, des1 = surf.detectAndCompute(image1, None, None)
    kps2, des2 = surf.detectAndCompute(image2, None, None)

    # 判断描述子数据类型，若不符合则进行数据转换
    if des1.dtype is not 'float32':
        des1 = des1.astype('float32')
    if des2.dtype is not 'float32':
        des2 = des2.astype('float32')

    # 创建FlannBasedMatcher对象
    matcher = cv.FlannBasedMatcher()

    # 特征点匹配
    matches = matcher.match(des1, des2, None)

    # 寻找距离的最大值和最小值
    matches_list = []
    for match in matches:
        matches_list.append(match.distance)
    max_dist = max(matches_list)
    min_dist = min(matches_list)

    # 设定阈值，筛选出合适的匹配点对
    good_matches = []
    for match in matches:
        if match.distance < 0.4 * max_dist:
            good_matches.append(match)

    # 输出匹配成功的特征点数目
    print('匹配成功的特征点数目为：{}，筛选后的特征点数目为：{}'.format(len(matches), len(good_matches)))

    # 绘制筛选前后的匹配结果
    result1 = cv.drawMatches(image1, kps1, image2, kps2, matches, None)
    result2 = cv.drawMatches(image1, kps1, image2, kps2, good_matches, None)

    # 展示结果
    cv.imshow('Matches', result1)
    cv.imshow('Good Matches', result2)
    cv.waitKey(0)
    cv.destroyAllWindows()
###############################只要涉及到cv.xfeatures2d()都是无法运行的。
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 读取图像
    image1 = cv.imread('./images/box.png')
    image2 = cv.imread('./images/box_in_scene.png')
    if image1 is None or image2 is None:
        print('Failed to read box.png or box_in_scene.png.')
        sys.exit()

    # 创建ORB对象
    orb = cv.ORB_create(1000, 1.2, 8, 31, 0, 2, cv.ORB_HARRIS_SCORE, 31, 20)
    # 分别计算image1，image2的ORB特征点和描述子
    kps1, des1 = orb.detectAndCompute(image1, None, None)
    kps2, des2 = orb.detectAndCompute(image2, None, None)

    # 创建BFMatcher对象
    bf = cv.BFMatcher(cv.NORM_HAMMING)#
    # 暴力匹配
    matches = bf.match(des1, des2)#之前一直不明白match与knnmatch的返回值到底是什么，查阅了一些资料才理解。
    # 其实二者都是返回的DMatch类型的数据结构。
    # 那么这个这个DMatch数据结构究竟是什么呢？
    # 它包含三个非常重要的数据分别是queryIdx，trainIdx，distance
    # 先说一下这三个分别是什么在演示其用途：
    # queryIdx：测试图像的特征点描述符的下标（第几个特征点描述符），同时也是描述符对应特征点的下标。
    # trainIdx：样本图像的特征点描述符下标, 同时也是描述符对应特征点的下标。
    # distance：代表这怡翠匹配的特征点描述符的欧式距离，数值越小也就说明俩个特征点越相近。
    # bf = cv.BFMatcher_create()
    # matches = bf.match(des1, des2)
    # for matche in matches:
    #     print(matche)
    #     print(matche.queryIdx)
    #     print(matche.trainIdx)
    #     print(matche.distance)
    # 输出的结果如下
    # < DMatch
    # 0x7f47e3aa6f50 >
    # 4220
    # 588
    # 149.05032348632812
    # < DMatch
    # 0x7f47e3aa6f70 >
    # 4221
    # 3089
    # 303.5638427734375
    #每个特征点本身也具有以下属性：.pt关键点坐标，.angle表示关键点方向，response表示响应强度，.size标书该点的直径大小。
    #演示pt
    # def match_demo(image1, image2):
    #     gray1 = cv.cvtColor(image1, cv.COLOR_BGR2GRAY)
    #
    #
    # gray2 = cv.cvtColor(image2, cv.COLOR_BGR2GRAY)
    # sift = cv.xfeatures2d.SIFT_create()
    # key1, des1 = sift.detectAndCompute(gray1, None)
    # key2, des2 = sift.detectAndCompute(gray2, None)
    #
    # bf = cv.BFMatcher_create()
    # matches = bf.match(des1, des2)
    # for matche in matches:
    #     print(key1[matche.queryIdx].pt)  # 表示第（matche.queryIdx）个特征点的坐标
    #Knnmatch与match的返回值类型一样，只不过一组返回的俩个DMatch类型：
    # matches = flann.knnMatch(des1, des2, k=2)
    # # matchesMask = [[0, 0] for i in range(len(matches))]
    # for i, matche in enumerate(matches):
    #     print(matche)
    # 返回值是
    # [ < DMatch 0x7f117af995f0 >, < DMatch0x7f117af99610 >]
    # [ < DMatch 0x7f117af99630 >, < DMatch0x7f117af99650 >]
    #这俩个DMatch数据类型是俩个与原图像特征点最接近的俩个特征点（match返回的是最匹配的）只有这俩个特征点的欧式距离小于一定值的时候才会认为匹配成功。
    # 列入：原图像特征点与俩个绿色苹果相匹配，那么就会认为这个特征点是lv苹果，但若与原图像最接近的匹配分别是一个绿苹果和一个红苹果，那么就会认为匹配是失败的，即没有相匹配的特征点。
    #集体的信息 见到CSDN的博文 https://blog.csdn.net/weixin_44072651/article/details/89262277

    # 查找最小汉明距离
    matches_list = []
    for match in matches:
        matches_list.append(match.distance)
    min_dist = min(matches_list)

    # 设定阈值，筛选出合适汉明距离的匹配点对
    good_matches = []
    for match in matches:
        if match.distance <= max(2.0 * min_dist, 20.0):
            good_matches.append(match)

    # 使用RANSAC算法筛选匹配结果
    # 获取关键点坐标
    src_kps = np.float32([kps1[i.queryIdx].pt for i in good_matches]).reshape(-1, 1, 2)
    dst_kps = np.float32([kps2[i.trainIdx].pt for i in good_matches]).reshape(-1, 1, 2)

    # 使用RANSAC算法筛选
    M, mask = cv.findHomography(src_kps, dst_kps, method=cv.RANSAC, ransacReprojThreshold=5.0)

    # 保存筛选后的匹配点对
    good_ransac = []
    for i in range(len(mask)):
        if mask[i] == 1:
            good_ransac.append(good_matches[i])
    
    # 绘制筛选前后的匹配结果
    result = cv.drawMatches(image1, kps1, image2, kps2, good_ransac, None)

    # 展示结果
    cv.imshow('RANSAC Matches', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
######################################### cv.findHomography()
#############
#############   CAPTER 10
#############
#########################################
import cv2 as cv
import numpy as np
if __name__ == '__main__':
    # 设置一个 2 维坐标和一个 3 维坐标
    point1 = np.array([[3, 6, 1.5]])
    point2 = np.array([[23, 32, 1]])

    # 非齐次坐标转齐次坐标
    point3 = cv.convertPointsToHomogeneous(point1)
    point4 = cv.convertPointsToHomogeneous(point2)

    # 齐次坐标转非齐次坐标
    point5 = cv.convertPointsFromHomogeneous(point1)
    point6 = cv.convertPointsFromHomogeneous(point2)

    # 输出结果
    print('非齐次坐标：{:<20}转为齐次坐标：{}'.format(str(point1), str(point3)))
    print('非齐次坐标：{:<20}转为齐次坐标：{}'.format(str(point2), str(point4)))

    print('齐次坐标：{:<22}转为非齐次坐标：{}'.format(str(point1), str(point5)))
    print('齐次坐标：{:<22}转为非齐次坐标：{}'.format(str(point2), str(point6)))
###################很奇怪的东西，齐次转换成非齐次维度减1，非齐次转换成齐次后维度加1，这个实现只需要在现有数字后面加1。
####齐次坐标转化为笛卡尔坐标的方法是前面n-1个坐标分量分别除以最后一个分量即可。
##齐次坐标就是用N+1维来代表N维坐标。
##############################################################################################cv.convertPointsToHomogeneous()的变换是通过内参矩阵可以将相机坐标系下任意的三
#维坐标映射到像素坐标系中，构建空间点与像素之间的映射关系。
##
import cv2 as cv
import numpy as np
import sys


def compute_points(img):
    # 转为灰度图像
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # 定义方格标定板内角点数目（行，列）
    board_size = (9, 6)
    # 计算方格标定板角点
    _, points = cv.findChessboardCorners(gray, board_size)
    # 细化角点坐标
    _, points = cv.find4QuadCornerSubpix(gray, points, (5, 5))
    return points


if __name__ == '__main__':
    # 生成棋盘格内角点的三维坐标
    obj_points = np.zeros((54, 3), np.float32)
    obj_points[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)#T表示转置的意思。
    # a
    # array([[1, 2, 3],
    #        [4, 5, 6],
    #        [7, 8, 9]])
    # a.T
    # array([[1, 4, 7],
    #        [2, 5, 8],
    #        [3, 6, 9]])
    ##############################
    # >> > mgrid[0:5, 0:5]
    #  array([[[0, 0, 0, 0, 0],
    #           [1, 1, 1, 1, 1],
    #         [2, 2, 2, 2, 2],
    #         [3, 3, 3, 3, 3],
    #         [4, 4, 4, 4, 4]],
    #         [[0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4]]])
    #mgrid[0:9, 0:6]的意思是创造两个二维数组，每个数组的大小相同（行数由start1:end1:step1决定，列数由start2:end2:step2决定）。其中第一个二维数组b是按列存储，
    # 即从start1到end1，然后第二列start1到end1，第三列…；第二个二维数组a是按行存储，第一行从start2到end2，第二行重复…

    print(obj_points.shape)
    obj_points = np.reshape(obj_points, (54, 1, 3))#这个函数应该是生成了棋盘焦点的索引。
    print(obj_points)

    # 计算棋盘格内角点的三维坐标及其在图像中的二维坐标
    all_obj_points = []
    all_points = []
    for i in range(1, 5):
        # 读取图像
        image = cv.imread('./images/left0{}.jpg'.format(i))
        if image is None:
            print('Failed to read left0{}.jpg.'.format(i))
            sys.exit()

        # 获取图像尺寸
        h, w = image.shape[:2]
        # 计算三维坐标
        all_obj_points.append(obj_points)
        # 计算二维坐标
        all_points.append(compute_points(image))

    # 计算相机内参矩阵和畸变系数
    _, cameraMatrix, distCoeffs, rvecs, tvecs = cv.calibrateCamera(all_obj_points, all_points, (w, h), None, None)
    print('内参矩阵为：\n{}'.format(cameraMatrix))#相机内参矩阵
    print('畸变系数为：\n{}'.format(distCoeffs))#相机畸变系数矩阵

    # 此结果在后面ProjectPoints.py函数中有使用到，此处先进行计算
    print('旋转向量为：\n{}'.format(rvecs))#机坐标系与世界坐标系之间的旋转向量
    print('平移向量为：\n{}'.format(tvecs))#相机坐标系与世界坐标系之间的平移向量
############################cv.calibrateCamera()是单目摄像的最开始重要的一环，没有这个标定是无法读取摄像头的信息以及之后的操作的。
#我们已知的是畸变后的图像，要得到没有畸变的图像就要通过畸变模型推导其映射关系。 真实图像 imgR 与 畸变图像 imgD 之间的关系为: imgR(U, V) = imgD(Ud, Vd) 。通过这个关系，找出所有的 imgR(U, V) 。(U, V) 映射到 (Ud, Vd)
#中的 (Ud, Vd) 往往不是整数 (U和V是整数，因为它是我们要组成图像的像素坐标位置，以这正常图像的坐标位置去求在畸变图像中的坐标位置，取出对应的像素值，这也是正常图像的像素值)。 但是畸变的像素往往不是整数，所以需要通过插值来进行
#求解
#原文链接：https://blog.csdn.net/darlingqiang/article/details/111559053
#这里容易有一个误解，以为去畸变是对畸变图像进行畸变逆变换得到无畸变图像，实际不是的，畸变模型太复杂了，很难求逆变换，所以是将无畸变图像进行畸变变换到原图像去获得对应像素值。
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 输入CalibrateCamera.py程序中得到的内参矩阵
    cameraMatrix = np.array([[532.01629758, 0, 332.17251924],
                             [0, 531.56515879, 233.38807482],
                             [0, 0, 1]])

    # 输入CalibrateCamera.py程序中得到的畸变系数
    distCoeffs = np.array([[-0.28518841, 0.08009721, 0.00127403, -0.00241511, 0.10657911]])

    # 依次校正图像
    for i in range(1, 5):
        # 读取图像
        img = cv.imread('./images/left0{}.jpg'.format(i))
        if img is None:
            print('Failed to read left0{}.jpg.'.format(i))
            sys.exit()

        # 获取图像尺寸
        h, w = img.shape[:2]

        # 校正图像（使用cv.initUndistortRectifyMap()函数和cv.remap()函数）
        map1, map2 = cv.initUndistortRectifyMap(cameraMatrix, distCoeffs, None, None, (w, h), 5)
        result1 = cv.remap(img, map1, map2, cv.INTER_LINEAR)

        # 校正图像（使用cv.undistort()函数）
        result2 = cv.undistort(img, cameraMatrix, distCoeffs, newCameraMatrix=None)

        # 展示结果
        cv.imshow('Origin', img)
        cv.imshow('Result_left0{}.jpg(Mode 1)'.format(i), result1)
        cv.imshow('Result_left0{}.jpg(Mode 2)'.format(i), result2)
        k = cv.waitKey(0)

        # 设置点击enter键继续，其它键退出
        if k == 13:
            cv.destroyAllWindows()
        else:
            sys.exit()
#####################################分别使用cv.undistort()(方法一）和cv.initUndistortRectifyMap()+cv.remap()两种方法来对图像进行矫畸。这才是这些坐标变换的真正意义。
import cv2 as cv
import numpy as np
import sys
if __name__ == '__main__':
    # 输入CalibrateCamera.py程序中得到的内参矩阵
    cameraMatrix = np.array([[532.01629758, 0, 332.17251924],
                             [0, 531.56515879, 233.38807482],
                             [0, 0, 1]])

    # 输入CalibrateCamera.py程序中得到的畸变系数
    distCoeffs = np.array([[-0.28518841, 0.08009721, 0.00127403, -0.00241511, 0.10657911]])

    # 输入CalibrateCamera.py程序中得到的相机坐标系与世界坐标系之间的旋转向量和平移向量
    rvecs = np.array([[0.16460723], [0.29404635], [0.01212824]])
    tvecs = np.array([[-2.6881551], [-4.27993647], [15.91970296]])

    # 读取图像
    img = cv.imread('./images/left01.jpg')
    if img is None:
        print('Failed to read left0{}.jpg.'.format(i))
        sys.exit()

    # 转为灰度图像
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # 定义放个标定板内角点数目（行，列）
    board_size = (9, 6)
    # 计算方格标定板角点
    _, points = cv.findChessboardCorners(gray, board_size)
    # 细化角点坐标
    _, points = cv.find4QuadCornerSubpix(gray, points, (5, 5))

    # 生成棋盘格内角点的三维坐标
    obj_points = np.zeros((54, 3), np.float32)
    obj_points[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)
    obj_points = np.reshape(obj_points, (54, 1, 3))

    # 根据三维坐标和相机与世界坐标系时间的关系估计内角点像素坐标
    points1, _ = cv.projectPoints(obj_points, rvecs, tvecs, cameraMatrix, distCoeffs)
    #是指根据相机的成像模型计算空间中三维点在图像二维平面中坐标的过程
    # 计算图像中内角点的真实坐标误差
    error = 0
    for j in range(len(points)):
        error += np.sqrt(np.power((points[j][0][0] - points1[j][0][0]), 2) + np.power((points[j][0][1] - points1[j][0][1]), 2))
    #power(x, y) 函数，计算 x 的 y 次方。
    #print(np.power([2,3,4], 3))
    #结果为[ 8 27 64]
    print('图像中内角点的真实坐标误差为：{}'.format(round(error / len(points), 6)))
########################################################################cv.projectPoints()用于计算世界坐标系中的三维点投影到像素坐标系中的二维坐标。
import cv2 as cv
import numpy as np
import sys


def compute_rvec(points1, points2, matrix, coeffs):
    # 用PnP算法计算旋转和平移向量
    _, rvec1, tvec1 = cv.solvePnP(points1, points2, matrix, coeffs)
    # 用PnP+Ransac算法计算旋转向量和平移向量
    _, rvec2, tvec2, inliers = cv.solvePnPRansac(points1, points2, matrix, coeffs)

    # 旋转向量转换为旋转矩阵
    rvec1_transport, _ = cv.Rodrigues(rvec1)
    rvec2_transport, _ = cv.Rodrigues(rvec2)

    # 输出结果
    print('世界坐标系变换到相机坐标系的旋转向量（cv.solvePnP）：\n', rvec1)
    print('对应旋转矩阵为：\n', rvec1_transport)
    print('世界坐标系变换到相机坐标系的旋转向量（cv.solvePnPRansac）：\n', rvec2)
    print('对应旋转矩阵为：\n', rvec2_transport)


if __name__ == '__main__':
    # 输入CalibrateCamera.py程序中得到的内参矩阵
    cameraMatrix = np.array([[532.01629758, 0, 332.17251924],
                             [0, 531.56515879, 233.38807482],
                             [0, 0, 1]])

    # 输入CalibrateCamera.py程序中得到的畸变系数
    distCoeffs = np.array([[-0.28518841, 0.08009721, 0.00127403, -0.00241511, 0.10657911]])

    # 生成棋盘格内角点的三维坐标
    obj_points = np.zeros((54, 3), np.float32)
    obj_points[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)
    obj_points = np.reshape(obj_points, (54, 1, 3))

    # 读取图像
    img = cv.imread('./images/left04.jpg')
    if img is None:
        print('Failed to read left04.jpg.')
        sys.exit()
    # 转为灰度图像
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # 定义放个标定板内角点数目（行，列）
    board_size = (9, 6)
    # 计算方格标定板角点
    _, points = cv.findChessboardCorners(gray, board_size)
    # 细化角点坐标
    _, points = cv.find4QuadCornerSubpix(gray, points, (5, 5))

    # 计算两个坐标系之间的旋转向量及旋转矩阵
    compute_rvec(obj_points, points, cameraMatrix, distCoeffs)

    # 修改其中一个三维坐标，重新进行计算
    obj_points[53] = [[8, 8, 0]]
    compute_rvec(obj_points, points, cameraMatrix, distCoeffs)
#根据相机成像模型，如果已知相机的内参矩阵、世界坐标系中若干三维点的三维坐标和空间点在图像中投影的二维坐标，就可以计算出世界坐标系到相机坐标系的旋转和
#平移向量。这就是PnP的方法。
####################################cv.solvePnPRansac()
import cv2 as cv
import numpy as np
import sys


def compute_points(img):
    # 转为灰度图像
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # 定义方格标定板内角点数目（行，列）
    board_size = (9, 6)#如果事先不知道这个行、列该如何办呢？
    # 计算方格标定板角点
    _, points = cv.findChessboardCorners(gray, board_size)
    # 细化角点坐标
    _, points = cv.find4QuadCornerSubpix(gray, points, (5, 5))
    return points


if __name__ == '__main__':
    # 生成棋盘格内角点的三维坐标
    obj_points = np.zeros((54, 3), np.float32)
    obj_points[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)
    obj_points = np.reshape(obj_points, (54, 1, 3)) * 10

    # 计算棋盘格内角点的三维坐标及其在图像中的二维坐标
    all_obj_points = []
    all_points_L = []
    all_points_R = []
    for i in range(1, 5):
        # 读取图像
        imageL = cv.imread('./images/left0{}.jpg'.format(i))
        if imageL is None:
            print('Failed to read left0{}.jpg.'.format(i))
            sys.exit()
        imageR = cv.imread('./images/right0{}.jpg'.format(i))
        if imageR is None:
            print('Failed to read right0{}.jpg.'.format(i))
            sys.exit()

        # 获取图像尺寸
        h, w = imageL.shape[:2]
        # 计算三维坐标
        all_obj_points.append(obj_points)
        # 计算二维坐标
        all_points_L.append(compute_points(imageL))
        all_points_R.append(compute_points(imageR))

    # 分别计算相机内参矩阵和畸变系数
    _, cameraMatrix1, distCoeffs1, rvecs1, tvecs1 = cv.calibrateCamera(all_obj_points, all_points_L, (w, h), None, None)
    _, cameraMatrix2, distCoeffs2, rvecs2, tvecs2 = cv.calibrateCamera(all_obj_points, all_points_R, (w, h), None, None)

    # 进行标定
    _, _, _, _, _, R, T, E, F = cv.stereoCalibrate(all_obj_points, all_points_L, all_points_R, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, (w, h), flags=cv.CALIB_USE_INTRINSIC_GUESS)

    # 展示结果
    print('两个相机坐标系的旋转矩阵：\n', R)
    print('两个相机坐标系的平移向量：\n', T)
############################################################这个函数类似于单目相机的求解旋转矩阵和平移矩阵的函数。这是双目相机的标定流程。他的作用在于知道畸变和内参矩阵。
import cv2 as cv
import numpy as np
import sys


def compute_points(img):
    # 转为灰度图像
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # 定义方格标定板内角点数目（行，列）
    board_size = (9, 6)
    # 计算方格标定板角点
    _, points = cv.findChessboardCorners(gray, board_size)
    # 细化角点坐标
    _, points = cv.find4QuadCornerSubpix(gray, points, (5, 5))
    return points


if __name__ == '__main__':
    # 生成棋盘格内角点的三维坐标
    obj_points = np.zeros((54, 3), np.float32)
    obj_points[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)
    obj_points = np.reshape(obj_points, (54, 1, 3)) * 10

    # 计算棋盘格内角点的三维坐标及其在图像中的二维坐标
    all_obj_points = []
    all_points_L = []
    all_points_R = []
    imageLs = []
    imageRs = []
    for i in range(1, 5):
        # 读取图像
        imageL = cv.imread('./images/left0{}.jpg'.format(i))
        if imageL is None:
            print('Failed to read left0{}.jpg.'.format(i))
            sys.exit()
        imageLs.append(imageL)
        imageR = cv.imread('./images/right0{}.jpg'.format(i))
        if imageR is None:
            print('Failed to read right0{}.jpg.'.format(i))
            sys.exit()
        imageRs.append(imageR)

        # 获取图像尺寸
        h, w = imageL.shape[:2]
        # 计算三维坐标
        all_obj_points.append(obj_points)
        # 计算二维坐标
        all_points_L.append(compute_points(imageL))
        all_points_R.append(compute_points(imageR))

    # 分别计算相机内参矩阵和畸变系数
    _, cameraMatrix1, distCoeffs1, rvecs1, tvecs1 = cv.calibrateCamera(all_obj_points, all_points_L, (w, h), None, None)
    _, cameraMatrix2, distCoeffs2, rvecs2, tvecs2 = cv.calibrateCamera(all_obj_points, all_points_R, (w, h), None, None)

    # 进行标定
    _, _, _, _, _, R, T, E, F = cv.stereoCalibrate(all_obj_points, all_points_L, all_points_R, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, (w, h), flags=cv.CALIB_USE_INTRINSIC_GUESS)

    # 计算校正变换矩阵
    R1, R2, P1, P2, Q, _, _ = cv.stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, (w, h), R, T, flags=0)

    # 计算校正映射矩阵
    mapL1, mapL2 = cv.initUndistortRectifyMap(cameraMatrix1, distCoeffs1, None, None, (w, h), 5)
    mapR1, mapR2 = cv.initUndistortRectifyMap(cameraMatrix2, distCoeffs2, None, None, (w, h), 5)

    # 校正
    for i in range(len(imageLs)):
        # 校正图像
        result1 = cv.remap(imageLs[i], mapL1, mapL2, cv.INTER_LINEAR)
        result2 = cv.remap(imageRs[i], mapR1, mapR2, cv.INTER_LINEAR)
        # 拼接图像（同样处理原图像以便做对比）
        origin = np.concatenate([imageLs[i], imageRs[i]], 1)
        result = np.concatenate([result1, result2], 1)
#np.concatenate()是将将数组拼接起来，就是将图片也拼接了起来。
#>>> a=np.array([1,2,3])
#>>> b=np.array([11,22,33])
#>>> c=np.array([44,55,66])
#>>> np.concatenate((a,b,c),axis=0)  # 默认情况下，axis=0可以不写
#array([ 1,  2,  3, 11, 22, 33, 44, 55, 66]) #对于一维数组拼接，axis的值不影响最后的结果
#>>> a=np.array([[1,2,3],[4,5,6]])
#>>> b=np.array([[11,21,31],[7,8,9]])
#>>> np.concatenate((a,b),axis=0)
#array([[ 1,  2,  3],
       [ 4,  5,  6],
       [11, 21, 31],
       [ 7,  8,  9]])
 
#>>> np.concatenate((a,b),axis=1)  #axis=1表示对应行的数组进行拼接
#array([[ 1,  2,  3, 11, 21, 31],
       [ 4,  5,  6,  7,  8,  9]])
        # 绘制直线，用于比较同一个内角点y轴是否一致
        origin = cv.line(origin, (-1, int(all_points_L[i][0][0][1])), (len(result[0]), int(all_points_L[i][0][0][1])), (0, 0, 255), 2)
        result = cv.line(result, (-1, int(all_points_L[i][0][0][1])), (len(result[0]), int(all_points_L[i][0][0][1])), (0, 0, 255), 2)
        # 展示结果
        cv.imshow('origin', origin)
        cv.imshow('result', result)
        k = cv.waitKey(0)
        # 设置点击enter键继续，其它键退出
        if k == 13:
            cv.destroyAllWindows()
        else:
            sys.exit()
#双目相机标定可以得到两个相机坐标系之间的变换关系，根据变换关系可以将两部相机的成像平面变换到同一个平面，同时图像的X轴共线这样的好处是空间中点的坐标在两张图像上的投影点具有相同的高度。
################
######
######CAPTER 11
######
###############
# -*- coding:utf-8 -*-
import cv2 as cv
import sys


if __name__ == '__main__':
    capture = cv.VideoCapture('./data/bike.avi')

    # 判断是否成功加载视频文件
    if not capture.isOpened():
        print('Failed to read bike.avi.')
        sys.exit()

    # 输出视频相关信息
    fps = capture.get(cv.CAP_PROP_FPS)
    width = capture.get(cv.CAP_PROP_FRAME_WIDTH)
    height = capture.get(cv.CAP_PROP_FRAME_HEIGHT)
    num_of_frames = capture.get(cv.CAP_PROP_FRAME_COUNT)
    print('视频宽度为：{}\n视频高度为：{}\n视频帧率为：{}\n视频总帧数为：{}'.format(width, height, fps, num_of_frames))

    # 读取视频中第一帧图像作为前一帧图像，并进行灰度化
    _, pre_frame = capture.read()
    pre_gray = cv.cvtColor(pre_frame, cv.COLOR_BGR2GRAY)
    # 对图像进行高斯滤波，减少噪声干扰
    pre_gray = cv.GaussianBlur(pre_gray, (0, 0), 15)

    # 生成形态学操作的矩阵模板
    kernel = cv.getStructuringElement(cv.MORPH_RECT, (7, 7), (-1, -1))
    while True:
        ret, frame = capture.read()
        # 当所有帧读取完毕后退出循环
        if ret is False:
            break
        else:
            # 对当前帧进行灰度化
            gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
            gray = cv.GaussianBlur(gray, (0, 0), 15)

            # 计算当前帧与前一帧的差值的绝对值
            res = cv.absdiff(gray, pre_gray)

            # 对结果进行二值化并进行开运算，以减少噪声干扰
            res = cv.threshold(res, 10, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
            res = cv.morphologyEx(res[1], cv.MORPH_OPEN, kernel)

            # 显示结果
            cv.imshow('Origin', frame)
            cv.imshow('Result', res)

            # 将当前帧变为前一帧（注释掉代表以第一帧为固定背景）
            pre_gray = gray.copy()

            # 设置延迟50毫秒，按ESC键退出
            if cv.waitKey(50) & 0xFF == 27:
                break
    # 释放并关闭窗口
    capture.release()
    cv.destroyAllWindows()
##############################################cv.absdiff()是用来比较两个图片的插值的绝对值。
import cv2 as cv
import sys


if __name__ == '__main__':
    capture = cv.VideoCapture('./data/vtest.avi')

    # 判断是否成功加载视频文件
    if not capture.isOpened():
        print('Failed to read vtest.avi.')
        sys.exit()

    # 选择跟踪区域
    _, frame = capture.read()
    x, y, w, h = cv.selectROI('MeanShift Demo', frame, True, False)
    track_window = (x, y, w, h)

    # 获取ROI直方图
    roi = frame[y: y+h, x: x+w]
    # 将图像转化为HSV颜色空间
    hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2GRAY)
    # 阈值操作
    mask = cv.inRange(hsv_roi, 0, 255)
    # 计算直方图和直方图归一化
    roi_hist = cv.calcHist([hsv_roi], [0], hsv_roi, [180], [0, 180])
    roi_hist = cv.normalize(roi_hist, None, 0, 255, cv.NORM_MINMAX)

    # 设置迭代算法终止条件
    criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)

    while True:
        ret, frame = capture.read()
        # 当所有帧读取完毕后退出循环
        if ret is False:
            break
        else:
            obj_hsv = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
            obj_hist = cv.calcBackProject([obj_hsv], [0], roi_hist, [0, 180], 1)#直方图反向映射。
            # 均值迁移，搜索更新roi区域
            ret, track_window = cv.meanShift(obj_hist, track_window, criteria)

            # 绘制跟踪结果
            x, y, w, h = track_window
            cv.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)
            cv.imshow('MeanShift Demo', frame)

            # 设置延迟50毫秒，按ESC键退出
            if cv.waitKey(50) & 0xff == 27:
                break

    # 释放并关闭窗口
    capture.release()
    cv.destroyAllWindows()
####################################使用直方图反射来跟踪选定的ROI区域。
import cv2 as cv
import sys
if __name__ == '__main__':
    capture = cv.VideoCapture('./data/mulballs.mp4')

    # 判断是否成功加载视频文件
    if not capture.isOpened():
        print('Failed to read mulballs.mp4.')
        sys.exit()

    # 选择跟踪区域
    _, frame = capture.read()
    x, y, w, h = cv.selectROI('CamShift Demo', frame, True, False)
    track_window = (x, y, w, h)

    # 获取ROI直方图
    roi = frame[y: y+h, x: x+w]
    # 将图像转化为HSV颜色空间
    hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2GRAY)
    # 阈值操作
    mask = cv.inRange(hsv_roi, 0, 255)
    # 计算直方图和直方图归一化
    roi_hist = cv.calcHist([hsv_roi], [0], hsv_roi, [180], [0, 180])
    roi_hist = cv.normalize(roi_hist, None, 0, 255, cv.NORM_MINMAX)

    # 设置迭代算法终止条件
    criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)

    while True:
        ret, frame = capture.read()
        if frame is None:
            pass
        else:
            frame1 = frame.copy()
        # 当所有帧读取完毕后退出循环
        if ret is False:
            break
        else:
            obj_hsv = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
            obj_hist = cv.calcBackProject([obj_hsv], [0], roi_hist, [0, 180], 1)
            # 自适应均值迁移，搜索更新roi区域
            ret, track_window = cv.CamShift(obj_hist, track_window, criteria)

            # 绘制跟踪结果
            x, y, w, h = track_window
            # 利用ret中的信息绘制椭圆形
            cv.ellipse(frame, ret, (0, 0, 255), thickness=2)
            # 利用track_window中的信息绘制矩形
            #cv.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)

            # 均值迁移，搜索更新roi区域
            ret, track_window = cv.meanShift(obj_hist, track_window, criteria)
            # 绘制跟踪结果
            x, y, w, h = track_window
            cv.rectangle(frame1, (x, y), (x + w, y + h), (0, 0, 255), 2)
            cv.imshow('CamShift Demo', frame)
            cv.imshow('MeanShift Demo', frame1)

            # 设置延迟50毫秒，按ESC键退出
            if cv.waitKey(50) & 0xff == 27:
                break

    # 释放并关闭窗口
    capture.release()
    cv.destroyAllWindows()
    #这个最奇怪的是所有的参数都是相同的，这样也可以显示在两个不同的图像中。
##################################
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    capture = cv.VideoCapture('./data/vtest.avi')

    # 判断是否成功加载视频文件
    if not capture.isOpened():
        print('Failed to read vtest.avi.')
        sys.exit()

    # 读取并处理第一帧图像作为函数使用的前一帧图像
    _, pre_frame = capture.read()
    pre_gray = cv.cvtColor(pre_frame, cv.COLOR_BGR2GRAY)

    # 初始化HSV图像
    hsv = np.zeros_like(pre_frame)#np.zeros_like(a)的目的是构建一个与a同维度的数组，并初始化所有变量为零。
    #print(f'hsv是{hsv}')
    hsv[..., 1] = 255#[..., 1]这种是每个1的位置都赋值为255
    #print(f'hsv是{hsv}')

    while True:
        _, next_frame = capture.read()
        next_gray = cv.cvtColor(next_frame, cv.COLOR_BGR2GRAY)
        # 计算稠密光流，计算图像中所有像素的运动速度
        flow = cv.calcOpticalFlowFarneback(pre_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        # 计算向量角度和幅值，
        magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])
        # 将角度转换成角度制
        hsv[..., 0] = angle * 180 / np.pi / 2#[..., 0]这种是每个1的位置都变成角度
        # 将幅值归一化到0-255区间便于显示结果
        hsv[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)#[..., 0]这种是每个1的位置都变成角度
        # 将HSV颜色空间图像转换到RGB颜色空间中
        result = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)

        # 展示原始图像和结果
        cv.imshow('Origin', next_frame)
        cv.imshow('Object Detect Result', result)

        # 设置延迟50毫秒，按ESC键退出
        if cv.waitKey(50) & 0xff == 27:
            break

    # 释放并关闭窗口
    capture.release()
    cv.destroyAllWindows()
##########################################其实可以看到HSV的作用一直伴随着整个流程。
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    capture = cv.VideoCapture('./data/mulballs.mp4')
    # 判断是否成功加载视频文件
    if not capture.isOpened():
        print('Failed to read mulballs.mp4.')
        sys.exit()

    # 随机选取颜色
    color = np.random.randint(0, 255, (100, 3))#打印出来，是（100，3）的数组，里面的每个数字都在0-255之间。
    print(color)
    #exit()

    # 读取第一帧
    _, pre_frame = capture.read()
    pre_gray = cv.cvtColor(pre_frame, cv.COLOR_BGR2GRAY)
    # 进行角点检测
    points = cv.goodFeaturesToTrack(pre_gray, maxCorners=5000, qualityLevel=0.01, minDistance=10,
                                    blockSize=3, useHarrisDetector=False, k=0.04)

    # 光流跟踪
    while True:
        ret, frame = capture.read()
        if ret is False:
            break
        frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
        # 稀疏光流检测
        criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 30, 0.01)
        next_pts, status, err = cv.calcOpticalFlowPyrLK(pre_gray, frame_gray, points, None, winSize=(31, 31),
                                                        maxLevel=3, criteria=criteria, flags=0)
        #next_pts 指的是当前帧中与前一帧图像稀疏光流点匹配成功的稀疏光流点坐标。
        #前一帧图像的稀疏光流点坐标
        #status 是输出状态向量
        # 根据状态对角点做筛选
        good_next = next_pts[status == 1]
        good_pre = points[status == 1]

        # 绘制跟踪线
        for i, (next_item, pre_item) in enumerate(zip(good_next, good_pre)):
            a, b = next_item.ravel()
            c, d = pre_item.ravel()
            #print(a,b,c,d)
            # 设置阈值，只绘制移动的角点
            dist = abs(a - c) + abs(b - d)
            if dist > 2:
                frame = cv.circle(frame, (int(a), int(b)), 3, color[i].tolist(), -1, 8)
                print(f'这里是color{color[i]}')
                frame = cv.line(frame, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2, 8, 0)

        # 展示结果
        cv.imshow('Result', frame)
        # 设置延迟50毫秒，按ESC键退出
        if cv.waitKey(50) & 0xff == 27:
            break

        # 更新前一帧图像和角点坐标
        pre_gray = frame_gray.copy()
        points = good_next.reshape(-1, 1, 2)

    # 释放并关闭窗口
    cv.destroyAllWindows()
    capture.release()
#######################################这是使用LK稀疏光流法
######
######            CAPTER 12
######
#######################################
import numpy as np
import cv2 as cv


if __name__ == '__main__':
    image = cv.imread('./images/people.jpg')
    # 判断是否成功读取图像
    if image is None:
        print('Failed to read people.jpg.')
        sys.exit()

    # 记录图像尺寸
    h, w, s = image.shape[::]

    # 定义一个用来填充分割后图像的色彩集合（注意此处的色彩数目不能少于图像分割的类别数）
    colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0), (255, 0, 255)]
    #print(colors[0])

    # 构建图像数据
    data = image.reshape((-1, 3))#为什么需要变形成这种格式呢？长宽两维合并了。
    data = np.float32(data)

    # 定义迭代算法终止条件
    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    # 设置图像分割的类别
    num_clusters = 3
    # 图像分割
    ret, labels, centers = cv.kmeans(data, num_clusters, None, criteria, num_clusters, cv.KMEANS_RANDOM_CENTERS)#可能是在kmesns算法中需要使用这种格式的图片。
    # 为不同类别的图像区域根据定义的颜色集合进行填色
    for i in range(len(data)):
        data[i] = colors[int(labels[i])]#
        #print(labels[i])
    print(i)

    # 展示结果
    result = data.reshape((h, w, s))
    cv.imshow('Origin', image)
    cv.imshow('Result', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
####################################################使用
