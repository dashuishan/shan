#######################
#####
#####  CAPTER 9
#####
#######################
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 转为灰度图像
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

    # 生成关键点
    kps = []
    key_points = np.random.randint(0, 512, 200).reshape((100, 2))
    # print(key_points)
    # exit()
    for point in key_points:
        kps.append(cv.KeyPoint(float(point[0]), float(point[1]), 1))#这里程序中代码错误，有对于cv.KeyPoint()，前两个参数需要用float类型。

    # 绘制关键点
    image_result = cv.drawKeypoints(image, kps, None, (), cv.DRAW_MATCHES_FLAGS_DEFAULT)
    gray_result = cv.drawKeypoints(gray, kps, None, (), cv.DRAW_MATCHES_FLAGS_DEFAULT)

    # 展示结果
    cv.imshow('Color Result', image_result)
    cv.imshow('Gray Result', gray_result)
    cv.waitKey(0)
    cv.destroyAllWindows()
#################这里的角点检测实在看不出是什么，因为这些角点都是随机生成的，根本不是关键点。
import cv2 as cv
import numpy as np
import sys

if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 转为灰度图像
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

    # 计算Harris系数
    harris = cv.cornerHarris(gray, 2, 3, 0.04, borderType=cv.BORDER_DEFAULT)

    # 对Harris进行归一化便于进行数值比较
    harris_nor = cv.normalize(harris, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX)
    harris_nor = harris_nor.astype('uint8')

    # 寻找Harris角点
    kps = []
    for i in np.argwhere(harris_nor > 125):#与125阈值相互比较，大于就保存下来。125这个值的选择，我认为与255的最大值关系较大。其实也可以该换成别的阈值。
        kps.append(cv.KeyPoint(float(i[1]), float(i[0]), 1))#则合理的cv.KeyPoint()里面是float属性。

    # 绘制角点
    result = cv.drawKeypoints(image, kps, None)

    # 展示结果
    cv.imshow('R', harris_nor)
    cv.imshow('Harris KeyPoints', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
#出Hams角点主要集中在图像人物的头发和帽子区域，因为这两个区域中线段的交点较多。
########################################################################梯度协方差矩阵的两个特征值与Harris角点判定相关。
import cv2 as cv
import numpy as np
import sys

if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()
    # 将图像进行拷贝便于使用cv.drawKeypoints()函数绘制角点
    image1 = image.copy()

    # 转为灰度图像，并将数据类型转换为float32
    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

    # 检测Shi-Tomasi角点
    corners = cv.goodFeaturesToTrack(gray, 500, 0.01, 0.04)
    corners = np.int0(corners)

    # 绘制角点（Mode 1：使用cv.circle()函数）
    kps = []
    for corner in corners:
        x, y = corner.ravel()#ravel()方法将数组维度拉成一维数组
        cv.circle(image, (x, y), 3, (0, 255, 255), -1)

        # 将角点转化为KeyPoint类，以便于方法2的绘制
        kps.append(cv.KeyPoint(float(x), float(y), 1))

    # 绘制角点（Mode 2：使用cv.drawKeypoints()函数）
    result = cv.drawKeypoints(image1, kps, None, (), cv.DRAW_MATCHES_FLAGS_DEFAULT)

    # 展示结果
    cv.imshow('Shi-Tomasi KeyPoints(Mode 1)', image)
    cv.imshow('Shi-Tomasi KeyPoints(Mode 2)', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
#########################################Shi-Tomasi角点检测方法是cornerHarris的一种。也是矩阵梯度协方差矩阵特征值的最小值，且能够使用 cv.drawKeypoints()函数直接画出来，不需要
阈值的限定了。
import cv2 as cv
import sys

if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 创建SURF对象
    surf = cv.xfeatures2d.SURF_create(500, 4, 3, True, False)

    # 计算SURF特征点
    kps = surf.detect(image, None)

    # 计算SURF描述子
    descriptions = surf.compute(image, kps)

    # 绘制SURF特征点
    image1 = image.copy()
    # 不含角度和大小
    image = cv.drawKeypoints(image, kps, image, ())
    # 包含角度和大小
    image1 = cv.drawKeypoints(image1, kps, image1, (), cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # 展示结果
    cv.imshow('SURF KeyPoints', image)
    cv.imshow('SURF KeyPoints(with Angle and Size)', image1)
    cv.waitKey(0)
    cv.destroyAllWindows()
##########################################################计算SURF特征点。但是cv.xfeatures2d.SURF_create()在新的版本中并不支持调用。SURF是SIFT的变形。方便提升SIFT的效率。
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 读取图像
    image = cv.imread('./images/lena.jpg')
    if image is None:
        print('Failed to read lena.jpg.')
        sys.exit()

    # 创建ORB对象
    orb = cv.ORB_create(500, 1.2, 8, 31, 0, 2, cv.ORB_HARRIS_SCORE, 31, 20)

    # 计算ORB特征点
    kps = orb.detect(image, None)#因为是继承的关系，所以是可以使用detect和compute方法的。

    # 计算ORB描述子
    descriptions = orb.compute(image, kps)

    # 绘制ORB特征点
    image1 = image.copy()
    # 不含角度和大小
    image = cv.drawKeypoints(image, kps, image, ())
    # 包含角度和大小
    image1 = cv.drawKeypoints(image1, kps, image1, (), cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # 展示结果
    cv.imshow('ORB KeyPoints', image)
    cv.imshow('ORB KeyPoints(with Angle and Size)', image1)
    cv.waitKey(0)
    cv.destroyAllWindows()
##################################################ORB存在的意义是SURF和SIFT的速率太低，而且专利限制也无法进行相关运算。这个是计算两个像素点之间汉明距的方法，而SURE和SIFT是计算欧式距离的方法。
####两个整数之间的 汉明距离 指的是这两个数字对应二进制位不同的位置的数目。
import cv2 as cv
import sys


if __name__ == '__main__':
    # 读取图像
    image1 = cv.imread('./images/box.png')
    image2 = cv.imread('./images/box_in_scene.png')
    if image1 is None or image2 is None:
        print('Failed to read box.png or box_in_scene.png.')
        sys.exit()

    # 创建ORB对象
    orb = cv.ORB_create(1000, 1.2, 8, 31, 0, 2, cv.ORB_HARRIS_SCORE, 31, 20)#暴力匹配使用的是ORB的方法，因此是可以运行的。

    # 分别计算image1，image2的ORB特征点和描述子
    kps1, des1 = orb.detectAndCompute(image1, None, None)#分别计算两张图片的ORB特征和描述子
    kps2, des2 = orb.detectAndCompute(image2, None, None)

    # 特征点匹配
    # 创建BFMatcher对象
    bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)#crossCheck意思是使用了交叉检查的方法
    matches = bf.match(des1, des2)

    # 输出匹配结果中最大值和最小值
    matches_list = []
    for match in matches:
        matches_list.append(match.distance)
    max_dist = max(matches_list)
    min_dist = min(matches_list)

    # 设定阈值，筛选出合适的匹配点对
    good_matches = []
    for match in matches:
        if match.distance <= max(2.0 * min_dist, 20.0):#距离最小汉明距离的2倍和距离20中的较大值作为优化匹配的阈值对特征点对进行筛选。
            good_matches.append(match)

    # 输出匹配成功的特征点数目
    print('匹配成功的特征点数目为：{}，筛选后的特征点数目为：{}'.format(len(matches), len(good_matches)))

    # 绘制筛选前后的匹配结果
    result1 = cv.drawMatches(image1, kps1, image2, kps2, matches, None)#cv.drawMatches()画出来匹配的线段
    result2 = cv.drawMatches(image1, kps1, image2, kps2, good_matches, None)

    # 展示结果
    cv.imshow('Matches', result1)
    cv.imshow('Good Matches', result2)
    cv.waitKey(0)
    cv.destroyAllWindows()
    ################这里是暴力匹配的案例，使用的ORB的匹配 然后使用了cv.BFMatcher()对比两张图。并使用cv.drawMatches()画出匹配的线段。
import cv2 as cv
import sys


if __name__ == '__main__':
    # 读取图像
    image1 = cv.imread('./images/box.png')
    image2 = cv.imread('./images/box_in_scene.png')
    if image1 is None or image2 is None:
        print('Failed to read box.png or box_in_scene.png.')
        sys.exit()

    # 创建ORB对象
    surf = cv.xfeatures2d.SURF_create(500, 4, 3, True, False)

    # 分别计算image1，image2的ORB特征点和描述子
    kps1, des1 = surf.detectAndCompute(image1, None, None)
    kps2, des2 = surf.detectAndCompute(image2, None, None)

    # 判断描述子数据类型，若不符合则进行数据转换
    if des1.dtype is not 'float32':
        des1 = des1.astype('float32')
    if des2.dtype is not 'float32':
        des2 = des2.astype('float32')

    # 创建FlannBasedMatcher对象
    matcher = cv.FlannBasedMatcher()

    # 特征点匹配
    matches = matcher.match(des1, des2, None)

    # 寻找距离的最大值和最小值
    matches_list = []
    for match in matches:
        matches_list.append(match.distance)
    max_dist = max(matches_list)
    min_dist = min(matches_list)

    # 设定阈值，筛选出合适的匹配点对
    good_matches = []
    for match in matches:
        if match.distance < 0.4 * max_dist:
            good_matches.append(match)

    # 输出匹配成功的特征点数目
    print('匹配成功的特征点数目为：{}，筛选后的特征点数目为：{}'.format(len(matches), len(good_matches)))

    # 绘制筛选前后的匹配结果
    result1 = cv.drawMatches(image1, kps1, image2, kps2, matches, None)
    result2 = cv.drawMatches(image1, kps1, image2, kps2, good_matches, None)

    # 展示结果
    cv.imshow('Matches', result1)
    cv.imshow('Good Matches', result2)
    cv.waitKey(0)
    cv.destroyAllWindows()
###############################只要涉及到cv.xfeatures2d()都是无法运行的。
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 读取图像
    image1 = cv.imread('./images/box.png')
    image2 = cv.imread('./images/box_in_scene.png')
    if image1 is None or image2 is None:
        print('Failed to read box.png or box_in_scene.png.')
        sys.exit()

    # 创建ORB对象
    orb = cv.ORB_create(1000, 1.2, 8, 31, 0, 2, cv.ORB_HARRIS_SCORE, 31, 20)
    # 分别计算image1，image2的ORB特征点和描述子
    kps1, des1 = orb.detectAndCompute(image1, None, None)
    kps2, des2 = orb.detectAndCompute(image2, None, None)

    # 创建BFMatcher对象
    bf = cv.BFMatcher(cv.NORM_HAMMING)#
    # 暴力匹配
    matches = bf.match(des1, des2)#之前一直不明白match与knnmatch的返回值到底是什么，查阅了一些资料才理解。
    # 其实二者都是返回的DMatch类型的数据结构。
    # 那么这个这个DMatch数据结构究竟是什么呢？
    # 它包含三个非常重要的数据分别是queryIdx，trainIdx，distance
    # 先说一下这三个分别是什么在演示其用途：
    # queryIdx：测试图像的特征点描述符的下标（第几个特征点描述符），同时也是描述符对应特征点的下标。
    # trainIdx：样本图像的特征点描述符下标, 同时也是描述符对应特征点的下标。
    # distance：代表这怡翠匹配的特征点描述符的欧式距离，数值越小也就说明俩个特征点越相近。
    # bf = cv.BFMatcher_create()
    # matches = bf.match(des1, des2)
    # for matche in matches:
    #     print(matche)
    #     print(matche.queryIdx)
    #     print(matche.trainIdx)
    #     print(matche.distance)
    # 输出的结果如下
    # < DMatch
    # 0x7f47e3aa6f50 >
    # 4220
    # 588
    # 149.05032348632812
    # < DMatch
    # 0x7f47e3aa6f70 >
    # 4221
    # 3089
    # 303.5638427734375
    #每个特征点本身也具有以下属性：.pt关键点坐标，.angle表示关键点方向，response表示响应强度，.size标书该点的直径大小。
    #演示pt
    # def match_demo(image1, image2):
    #     gray1 = cv.cvtColor(image1, cv.COLOR_BGR2GRAY)
    #
    #
    # gray2 = cv.cvtColor(image2, cv.COLOR_BGR2GRAY)
    # sift = cv.xfeatures2d.SIFT_create()
    # key1, des1 = sift.detectAndCompute(gray1, None)
    # key2, des2 = sift.detectAndCompute(gray2, None)
    #
    # bf = cv.BFMatcher_create()
    # matches = bf.match(des1, des2)
    # for matche in matches:
    #     print(key1[matche.queryIdx].pt)  # 表示第（matche.queryIdx）个特征点的坐标
    #Knnmatch与match的返回值类型一样，只不过一组返回的俩个DMatch类型：
    # matches = flann.knnMatch(des1, des2, k=2)
    # # matchesMask = [[0, 0] for i in range(len(matches))]
    # for i, matche in enumerate(matches):
    #     print(matche)
    # 返回值是
    # [ < DMatch 0x7f117af995f0 >, < DMatch0x7f117af99610 >]
    # [ < DMatch 0x7f117af99630 >, < DMatch0x7f117af99650 >]
    #这俩个DMatch数据类型是俩个与原图像特征点最接近的俩个特征点（match返回的是最匹配的）只有这俩个特征点的欧式距离小于一定值的时候才会认为匹配成功。
    # 列入：原图像特征点与俩个绿色苹果相匹配，那么就会认为这个特征点是lv苹果，但若与原图像最接近的匹配分别是一个绿苹果和一个红苹果，那么就会认为匹配是失败的，即没有相匹配的特征点。
    #集体的信息 见到CSDN的博文 https://blog.csdn.net/weixin_44072651/article/details/89262277

    # 查找最小汉明距离
    matches_list = []
    for match in matches:
        matches_list.append(match.distance)
    min_dist = min(matches_list)

    # 设定阈值，筛选出合适汉明距离的匹配点对
    good_matches = []
    for match in matches:
        if match.distance <= max(2.0 * min_dist, 20.0):
            good_matches.append(match)

    # 使用RANSAC算法筛选匹配结果
    # 获取关键点坐标
    src_kps = np.float32([kps1[i.queryIdx].pt for i in good_matches]).reshape(-1, 1, 2)
    dst_kps = np.float32([kps2[i.trainIdx].pt for i in good_matches]).reshape(-1, 1, 2)

    # 使用RANSAC算法筛选
    M, mask = cv.findHomography(src_kps, dst_kps, method=cv.RANSAC, ransacReprojThreshold=5.0)

    # 保存筛选后的匹配点对
    good_ransac = []
    for i in range(len(mask)):
        if mask[i] == 1:
            good_ransac.append(good_matches[i])
    
    # 绘制筛选前后的匹配结果
    result = cv.drawMatches(image1, kps1, image2, kps2, good_ransac, None)

    # 展示结果
    cv.imshow('RANSAC Matches', result)
    cv.waitKey(0)
    cv.destroyAllWindows()
######################################### cv.findHomography()
#############
#############   CAPTER 10
#############
#########################################
import cv2 as cv
import numpy as np
if __name__ == '__main__':
    # 设置一个 2 维坐标和一个 3 维坐标
    point1 = np.array([[3, 6, 1.5]])
    point2 = np.array([[23, 32, 1]])

    # 非齐次坐标转齐次坐标
    point3 = cv.convertPointsToHomogeneous(point1)
    point4 = cv.convertPointsToHomogeneous(point2)

    # 齐次坐标转非齐次坐标
    point5 = cv.convertPointsFromHomogeneous(point1)
    point6 = cv.convertPointsFromHomogeneous(point2)

    # 输出结果
    print('非齐次坐标：{:<20}转为齐次坐标：{}'.format(str(point1), str(point3)))
    print('非齐次坐标：{:<20}转为齐次坐标：{}'.format(str(point2), str(point4)))

    print('齐次坐标：{:<22}转为非齐次坐标：{}'.format(str(point1), str(point5)))
    print('齐次坐标：{:<22}转为非齐次坐标：{}'.format(str(point2), str(point6)))
###################很奇怪的东西，齐次转换成非齐次维度减1，非齐次转换成齐次后维度加1，这个实现只需要在现有数字后面加1。
##############################################################################################cv.convertPointsToHomogeneous()的变换是通过内参矩阵可以将相机坐标系下任意的三
#维坐标映射到像素坐标系中，构建空间点与像素之间的映射关系。
##
import cv2 as cv
import numpy as np
import sys


def compute_points(img):
    # 转为灰度图像
    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    # 定义方格标定板内角点数目（行，列）
    board_size = (9, 6)
    # 计算方格标定板角点
    _, points = cv.findChessboardCorners(gray, board_size)
    # 细化角点坐标
    _, points = cv.find4QuadCornerSubpix(gray, points, (5, 5))
    return points


if __name__ == '__main__':
    # 生成棋盘格内角点的三维坐标
    obj_points = np.zeros((54, 3), np.float32)
    obj_points[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)#T表示转置的意思。
    # a
    # array([[1, 2, 3],
    #        [4, 5, 6],
    #        [7, 8, 9]])
    # a.T
    # array([[1, 4, 7],
    #        [2, 5, 8],
    #        [3, 6, 9]])
    ##############################
    # >> > mgrid[0:5, 0:5]
    #  array([[[0, 0, 0, 0, 0],
    #           [1, 1, 1, 1, 1],
    #         [2, 2, 2, 2, 2],
    #         [3, 3, 3, 3, 3],
    #         [4, 4, 4, 4, 4]],
    #         [[0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4],
    #         [0, 1, 2, 3, 4]]])
    #mgrid[0:9, 0:6]的意思是创造两个二维数组，每个数组的大小相同（行数由start1:end1:step1决定，列数由start2:end2:step2决定）。其中第一个二维数组b是按列存储，
    # 即从start1到end1，然后第二列start1到end1，第三列…；第二个二维数组a是按行存储，第一行从start2到end2，第二行重复…

    print(obj_points.shape)
    obj_points = np.reshape(obj_points, (54, 1, 3))#这个函数应该是生成了棋盘焦点的索引。
    print(obj_points)

    # 计算棋盘格内角点的三维坐标及其在图像中的二维坐标
    all_obj_points = []
    all_points = []
    for i in range(1, 5):
        # 读取图像
        image = cv.imread('./images/left0{}.jpg'.format(i))
        if image is None:
            print('Failed to read left0{}.jpg.'.format(i))
            sys.exit()

        # 获取图像尺寸
        h, w = image.shape[:2]
        # 计算三维坐标
        all_obj_points.append(obj_points)
        # 计算二维坐标
        all_points.append(compute_points(image))

    # 计算相机内参矩阵和畸变系数
    _, cameraMatrix, distCoeffs, rvecs, tvecs = cv.calibrateCamera(all_obj_points, all_points, (w, h), None, None)
    print('内参矩阵为：\n{}'.format(cameraMatrix))#相机内参矩阵
    print('畸变系数为：\n{}'.format(distCoeffs))#相机畸变系数矩阵

    # 此结果在后面ProjectPoints.py函数中有使用到，此处先进行计算
    print('旋转向量为：\n{}'.format(rvecs))#机坐标系与世界坐标系之间的旋转向量
    print('平移向量为：\n{}'.format(tvecs))#相机坐标系与世界坐标系之间的平移向量
############################
import cv2 as cv
import numpy as np
import sys


if __name__ == '__main__':
    # 输入CalibrateCamera.py程序中得到的内参矩阵
    cameraMatrix = np.array([[532.01629758, 0, 332.17251924],
                             [0, 531.56515879, 233.38807482],
                             [0, 0, 1]])

    # 输入CalibrateCamera.py程序中得到的畸变系数
    distCoeffs = np.array([[-0.28518841, 0.08009721, 0.00127403, -0.00241511, 0.10657911]])

    # 依次校正图像
    for i in range(1, 5):
        # 读取图像
        img = cv.imread('./images/left0{}.jpg'.format(i))
        if img is None:
            print('Failed to read left0{}.jpg.'.format(i))
            sys.exit()

        # 获取图像尺寸
        h, w = img.shape[:2]

        # 校正图像（使用cv.initUndistortRectifyMap()函数和cv.remap()函数）
        map1, map2 = cv.initUndistortRectifyMap(cameraMatrix, distCoeffs, None, None, (w, h), 5)
        result1 = cv.remap(img, map1, map2, cv.INTER_LINEAR)

        # 校正图像（使用cv.undistort()函数）
        result2 = cv.undistort(img, cameraMatrix, distCoeffs, newCameraMatrix=None)

        # 展示结果
        cv.imshow('Origin', img)
        cv.imshow('Result_left0{}.jpg(Mode 1)'.format(i), result1)
        cv.imshow('Result_left0{}.jpg(Mode 2)'.format(i), result2)
        k = cv.waitKey(0)

        # 设置点击enter键继续，其它键退出
        if k == 13:
            cv.destroyAllWindows()
        else:
            sys.exit()
#####################################分别使用cv.undistort()(方法一）和cv.initUndistortRectifyMap()+cv.remap()两种方法来对图像进行矫畸。这才是这些坐标变换的真正意义。
